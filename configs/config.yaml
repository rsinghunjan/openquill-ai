base_model: "meta-llama/Llama-2-7b-chat"  # change as desired
adapter_type: "blip2"
use_qlora: true
lora_r: 8
train_batch_size: 64
gradient_accumulation_steps: 8
max_seq_len: 2048
embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
faiss_index_path: "indexes/faiss_index.idx"
quant_bits: 4
device: "cuda"
