From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: OpenQuill RLHF Ops <rlhf-ops@example.com>
Date: 2026-02-17 00:00:00 +0000
Subject: [PATCH] RLHF productionization: annotator admin, reward holdout validation, PPO with reward shaping + HIL gating, QA dashboard, reward shaping config, promote gate and operational playbook

---
 tools/annotation_admin.py                             | 235 ++++++++++++++++++++++++++++++
 openquill/safety/reward_holdout.py                    | 303 +++++++++++++++++++++++++++++++++++++++
 openquill/training/ppo_shaped.py                      | 436 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 tools/qa_dashboard_stub.py                            | 126 +++++++++++++++++++
 configs/reward_shaping.yaml                           |  42 ++++++
 scripts/promote_policy_gate.sh                        | 153 +++++++++++++++++++++
 docs/rhf_operational_playbook.md                      | 216 +++++++++++++++++++++++++++++++
 7 files changed, 1511 insertions(+)
 create mode 100644 tools/annotation_admin.py
 create mode 100644 openquill/safety/reward_holdout.py
 create mode 100644 openquill/training/ppo_shaped.py
 create mode 100644 tools/qa_dashboard_stub.py
 create mode 100644 configs/reward_shaping.yaml
 create mode 100755 scripts/promote_policy_gate.sh
 create mode 100644 docs/rhf_operational_playbook.md
--- /dev/null
+++ b/tools/annotation_admin.py
@@ -0,0 +1,235 @@
+#!/usr/bin/env python3
+"""
+tools/annotation_admin.py
+
+Lightweight admin utilities for operating annotation programs at scale:
+ - manage annotator onboarding CSVs and tokens
+ - manage gold-test items and generate gold-test export for annotation service
+ - compute per-annotator metrics and produce QA reports
+ - mark/remove bad annotations
+
+This complements the annotation_service_fastapi and annotator_onboard helper.
+"""
+from __future__ import annotations
+import argparse
+import csv
+import json
+import secrets
+from pathlib import Path
+from typing import List, Dict, Any
+
+
+def generate_onboard_csv(count: int, out_path: str) -> None:
+    """
+    Generate annotator onboarding CSV with one-time tokens.
+    """
+    rows = []
+    for _ in range(count):
+        aid = f"ann_{secrets.token_hex(6)}"
+        token = secrets.token_urlsafe(24)
+        rows.append({"annotator_id": aid, "token": token, "email": f"{aid}@example.com"})
+    p = Path(out_path)
+    p.parent.mkdir(parents=True, exist_ok=True)
+    with p.open("w", newline="", encoding="utf-8") as f:
+        writer = csv.DictWriter(f, fieldnames=["annotator_id", "token", "email"])
+        writer.writeheader()
+        writer.writerows(rows)
+    print(f"Wrote {len(rows)} onboarding rows to {out_path}")
+
+
+def load_annotations_csv(path: str) -> List[Dict[str, Any]]:
+    rows = []
+    with open(path, "r", encoding="utf-8") as f:
+        reader = csv.DictReader(f)
+        for r in reader:
+            rows.append(r)
+    return rows
+
+
+def compute_annotator_metrics(annotations_csv: str) -> Dict[str, Any]:
+    """
+    Compute metrics per annotator: counts, avg confidence, gold pass rate (if gold present in notes or a gold column).
+    """
+    rows = load_annotations_csv(annotations_csv)
+    from collections import defaultdict
+    agg = defaultdict(lambda: {"count": 0, "sum_conf": 0.0, "gold_total": 0, "gold_correct": 0})
+    for r in rows:
+        a = r.get("annotator", "unknown")
+        agg[a]["count"] += 1
+        try:
+            agg[a]["sum_conf"] += float(r.get("confidence") or 3.0)
+        except Exception:
+            agg[a]["sum_conf"] += 3.0
+        # identify gold tests: if notes contain "gold:true" and "gold:pass" markers (convention)
+        notes = (r.get("notes") or "").lower()
+        if "gold:true" in notes:
+            agg[a]["gold_total"] += 1
+            if "gold:pass" in notes:
+                agg[a]["gold_correct"] += 1
+
+    out = {}
+    for k, v in agg.items():
+        out[k] = {
+            "count": v["count"],
+            "avg_confidence": (v["sum_conf"] / v["count"]) if v["count"] else None,
+            "gold_total": v["gold_total"],
+            "gold_correct": v["gold_correct"],
+            "gold_pass_rate": (v["gold_correct"] / v["gold_total"]) if v["gold_total"] else None,
+        }
+    return out
+
+
+def export_gold_for_service(gold_json_in: str, out_json: str) -> None:
+    """
+    Convert a gold tests JSON (list of {prompt, preferred}) into the export file expected by the annotation service.
+    """
+    p_in = Path(gold_json_in)
+    if not p_in.exists():
+        raise FileNotFoundError(gold_json_in)
+    data = json.loads(p_in.read_text(encoding="utf-8"))
+    # normalize and write
+    out_list = []
+    for e in data:
+        prompt = e.get("prompt")
+        preferred = e.get("preferred")
+        if prompt and preferred:
+            out_list.append({"prompt": prompt, "response_chosen": e.get("response_chosen", ""), "response_rejected": e.get("response_rejected", ""), "preferred": preferred})
+    Path(out_json).write_text(json.dumps(out_list, indent=2), encoding="utf-8")
+    print(f"Wrote {len(out_list)} gold items to {out_json}")
+
+
+def remove_bad_annotations(annotations_csv: str, output_csv: str, filter_fn=None) -> None:
+    """
+    Remove or redact annotations matching filter_fn from the CSV.
+    filter_fn receives a row dict and returns True to remove.
+    """
+    rows = load_annotations_csv(annotations_csv)
+    kept = []
+    for r in rows:
+        if filter_fn and filter_fn(r):
+            continue
+        kept.append(r)
+    with open(output_csv, "w", newline="", encoding="utf-8") as f:
+        writer = csv.DictWriter(f, fieldnames=list(kept[0].keys()) if kept else [])
+        if kept:
+            writer.writeheader()
+            writer.writerows(kept)
+    print(f"Wrote cleaned annotations to {output_csv} (kept {len(kept)} rows)")
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--generate-onboard", type=int, default=0, help="Generate onboarding rows")
+    parser.add_argument("--out", default="annotations/annotators.csv")
+    parser.add_argument("--metrics", default="", help="Compute annotator metrics from CSV")
+    parser.add_argument("--metrics-out", default="annotations/annotator_metrics.json")
+    parser.add_argument("--export-gold", default="", help="Export gold tests JSON -> service input")
+    parser.add_argument("--export-gold-out", default="annotations/gold_export.json")
+    parser.add_argument("--remove-bad", nargs=2, metavar=("in.csv", "out.csv"), help="Remove annotations matching filter; see code to customize filter")
+    args = parser.parse_args()
+
+    if args.generate_onboard:
+        generate_onboard_csv(args.generate_onboard, args.out)
+        return
+    if args.metrics:
+        m = compute_annotator_metrics(args.metrics)
+        Path(args.metrics_out).write_text(json.dumps(m, indent=2))
+        print("Wrote metrics to", args.metrics_out)
+        return
+    if args.export_gold:
+        export_gold_for_service(args.export_gold, args.export_gold_out)
+        return
+    if args.remove_bad:
+        inp, out = args.remove_bad
+        # sample filter: remove annotations with notes containing 'spam' or 'low_quality'
+        def fn(r):
+            n = (r.get("notes") or "").lower()
+            return ("spam" in n) or ("low_quality" in n)
+        remove_bad_annotations(inp, out, filter_fn=fn)
+        return
+
+    parser.print_help()
+
+
+if __name__ == "__main__":
+    main()
+
--- /dev/null
+++ b/openquill/safety/reward_holdout.py
@@ -0,0 +1,303 @@
+"""
+openquill/safety/reward_holdout.py
+
+Utilities to split pairwise preference data into train/holdout sets, train a reward model,
+and validate reward model quality on holdout data. Designed to be used as part of RLHF productionization.
+
+Features:
+ - deterministic split (seeded) into train/val/test
+ - reward model training via HF Trainer (small transformer)
+ - evaluation metrics: ROC-AUC, accuracy, calibration (Brier)
+ - save evaluation report JSON for release gates
+"""
+from __future__ import annotations
+import argparse
+import json
+import math
+import random
+from pathlib import Path
+from typing import Dict, List, Tuple
+
+import numpy as np
+import torch
+from datasets import Dataset
+from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
+
+
+def read_pairwise_jsonl(path: str) -> List[Dict]:
+    out = []
+    with open(path, "r", encoding="utf-8") as f:
+        for ln in f:
+            try:
+                out.append(json.loads(ln))
+            except Exception:
+                continue
+    return out
+
+
+def split_pairs(pairs: List[Dict], seed: int = 42, train_frac: float = 0.8, val_frac: float = 0.1) -> Tuple[List[Dict], List[Dict], List[Dict]]:
+    random.seed(seed)
+    shuffled = pairs[:]
+    random.shuffle(shuffled)
+    n = len(shuffled)
+    n_train = int(train_frac * n)
+    n_val = int(val_frac * n)
+    train = shuffled[:n_train]
+    val = shuffled[n_train:n_train + n_val]
+    test = shuffled[n_train + n_val:]
+    return train, val, test
+
+
+def pairwise_to_examples(pairs: List[Dict]) -> List[Dict]:
+    examples = []
+    for p in pairs:
+        examples.append({"text": p["prompt"] + " " + p["response_chosen"], "label": 1})
+        examples.append({"text": p["prompt"] + " " + p["response_rejected"], "label": 0})
+    return examples
+
+
+def train_reward(examples: List[Dict], model_name: str = "distilbert-base-uncased", output_dir: str = "./outputs/reward", epochs: int = 3, batch_size: int = 8) -> str:
+    ds = Dataset.from_list(examples)
+    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
+
+    def tok_fn(ex):
+        return tokenizer(ex["text"], truncation=True, padding="max_length", max_length=256)
+
+    ds_tok = ds.map(tok_fn, batched=True)
+    ds_tok = ds_tok.rename_column("label", "labels")
+    ds_tok.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
+
+    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)
+    args = TrainingArguments(output_dir=output_dir, per_device_train_batch_size=batch_size, num_train_epochs=epochs, logging_steps=10, save_total_limit=2)
+    trainer = Trainer(model=model, args=args, train_dataset=ds_tok)
+    trainer.train()
+    trainer.save_model(output_dir)
+    return output_dir
+
+
+def eval_reward_model(model_dir: str, examples: List[Dict], batch_size: int = 16) -> Dict:
+    from sklearn.metrics import roc_auc_score, accuracy_score, brier_score_loss
+    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)
+    model = AutoModelForSequenceClassification.from_pretrained(model_dir)
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    model.to(device)
+    texts = [e["text"] for e in examples]
+    labels = np.array([e["label"] for e in examples])
+    probs = []
+    for i in range(0, len(texts), batch_size):
+        batch = texts[i:i+batch_size]
+        toks = tokenizer(batch, truncation=True, padding=True, return_tensors="pt", max_length=256).to(device)
+        with torch.no_grad():
+            out = model(**toks)
+            logits = out.logits.squeeze(-1).cpu().numpy()
+            p = 1 / (1 + np.exp(-logits))
+            probs.extend(p.tolist())
+    probs = np.array(probs)
+    # compute metrics
+    auc = roc_auc_score(labels, probs) if len(np.unique(labels)) > 1 else None
+    preds = (probs >= 0.5).astype(int)
+    acc = accuracy_score(labels, preds)
+    brier = brier_score_loss(labels, probs)
+    return {"roc_auc": auc, "accuracy": float(acc), "brier": float(brier), "n": len(labels)}
+
+
+def run_holdout_pipeline(pairs_jsonl: str, output_dir: str, seed: int = 42, model_name: str = "distilbert-base-uncased"):
+    pairs = read_pairwise_jsonl(pairs_jsonl)
+    if not pairs:
+        raise RuntimeError("No pairs found in " + pairs_jsonl)
+    train_pairs, val_pairs, test_pairs = split_pairs(pairs, seed=seed)
+    train_ex = pairwise_to_examples(train_pairs)
+    val_ex = pairwise_to_examples(val_pairs)
+    test_ex = pairwise_to_examples(test_pairs)
+    print(f"Train/Val/Test sizes (pairs): {len(train_pairs)}/{len(val_pairs)}/{len(test_pairs)}")
+
+    reward_out = Path(output_dir)
+    reward_out.mkdir(parents=True, exist_ok=True)
+    # Train on train_ex (we could include val for early stopping; keep simple)
+    model_dir = train_reward(train_ex, model_name=model_name, output_dir=str(reward_out / "reward_model"), epochs=3)
+    # Evaluate on val and test
+    val_report = eval_reward_model(model_dir, val_ex)
+    test_report = eval_reward_model(model_dir, test_ex)
+    report = {"model_dir": model_dir, "val": val_report, "test": test_report, "seed": seed}
+    (reward_out / "reward_holdout_report.json").write_text(json.dumps(report, indent=2))
+    print("Wrote reward_holdout_report.json")
+    return report
+
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--pairs", default="data/reward_pairs.jsonl")
+    parser.add_argument("--out", default="outputs/reward_holdout")
+    parser.add_argument("--model_name", default="distilbert-base-uncased")
+    args = parser.parse_args()
+    r = run_holdout_pipeline(args.pairs, args.out, model_name=args.model_name)
+    print(json.dumps(r, indent=2))
+
--- /dev/null
+++ b/openquill/training/ppo_shaped.py
@@ -0,0 +1,436 @@
+"""
+openquill/training/ppo_shaped.py
+
+PPO runner with reward shaping and conservative safety penalties.
+ - Uses trl.PPOTrainer where available
+ - Loads a reward model (HF) and an external moderation adapter to apply penalties
+ - Logs rollouts to a JSONL file for human review
+ - Enqueues flagged rollouts to the HIL queue for human adjudication
+
+Usage (toy, local):
+  python openquill/training/ppo_shaped.py --policy distilgpt2 --prompts prompts.txt --reward_model outputs/reward --out outputs/ppo_shaped
+
+Caveats:
+ - trl/trlX API changes frequently; this script tries to be robust but may need adjustments for your trl version.
+ - Always run small pilots first. Do not promote PPO-updated policies to production without HIL review and safety signoff.
+"""
+from __future__ import annotations
+import argparse
+import json
+import logging
+import os
+from pathlib import Path
+from typing import List, Dict, Any, Optional
+
+import numpy as np
+import torch
+from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
+
+logger = logging.getLogger("openquill.ppo_shaped")
+logging.basicConfig(level=logging.INFO)
+
+try:
+    from trl import PPOTrainer, PPOConfig
+except Exception:
+    PPOTrainer = None
+    PPOConfig = None
+
+from openquill.safety.external_moderation import moderate as external_moderate
+from openquill.safety.hil_queue import enqueue as enqueue_hil
+
+
+def load_prompts(path: str) -> List[str]:
+    p = Path(path)
+    if not p.exists():
+        return ["Write a short summary about OpenQuill.", "Write a python function to add two numbers."]
+    return [l.strip() for l in p.read_text().splitlines() if l.strip()]
+
+
+def load_models(policy_name: str, reward_model_dir: str):
+    tok_policy = AutoTokenizer.from_pretrained(policy_name, use_fast=True)
+    policy = AutoModelForCausalLM.from_pretrained(policy_name)
+    tok_reward = AutoTokenizer.from_pretrained(reward_model_dir, use_fast=True)
+    reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_dir)
+    device = "cuda" if torch.cuda.is_available() else "cpu"
+    policy.to(device)
+    reward_model.to(device)
+    policy.eval()
+    reward_model.eval()
+    return {"policy": policy, "tok_policy": tok_policy, "reward_model": reward_model, "tok_reward": tok_reward, "device": device}
+
+
+def apply_reward_shaping(base_reward: float, generated_text: str, shaping_cfg: Dict[str, Any]) -> float:
+    """
+    Apply conservative shaping penalties:
+      - toxicity penalty using external moderation
+      - length penalty or repetition penalty if desired
+    shaping_cfg example:
+      toxicity_penalty: -5.0
+      repetition_penalty: -1.0
+    """
+    shaped = float(base_reward)
+    # toxicity check (local heuristic + external API)
+    try:
+        mod = external_moderate(generated_text, use_hf=False, use_perspective=False)
+    except Exception:
+        mod = {"safe": True}
+    if not mod.get("safe", True):
+        tox_pen = shaping_cfg.get("toxicity_penalty", -5.0)
+        shaped += tox_pen
+    # length penalty (optional)
+    max_len = shaping_cfg.get("max_len", 512)
+    tok_len = len(generated_text.split())
+    if tok_len > max_len:
+        shaped += shaping_cfg.get("length_penalty", -1.0)
+    return shaped
+
+
+def ppo_train(prompts: List[str], policy_name: str, reward_model_dir: str, out_dir: str, shaping_cfg: Dict[str, Any], ppo_epochs: int = 1, batch_size: int = 1):
+    """
+    Run PPO with shaping. This function logs rollouts and enqueues flagged ones to HIL.
+    """
+    models = load_models(policy_name, reward_model_dir)
+    policy = models["policy"]
+    tok_policy = models["tok_policy"]
+    reward_model = models["reward_model"]
+    tok_reward = models["tok_reward"]
+    device = models["device"]
+
+    # Initialize PPOTrainer if available
+    if PPOTrainer is None:
+        raise RuntimeError("trl package not installed. Install with `pip install trl`")
+    ppo_cfg = PPOConfig(model_name=policy_name, learning_rate=1.41e-5, batch_size=batch_size, ppo_epochs=1)
+    trainer = PPOTrainer(ppo_cfg, model=policy, tokenizer=tok_policy)
+
+    out_path = Path(out_dir)
+    out_path.mkdir(parents=True, exist_ok=True)
+    rollouts_file = out_path / "rollouts.jsonl"
+
+    def score_with_reward(prompt: str, response: str) -> float:
+        # base reward from reward model
+        inp = tok_reward(prompt + " " + response, return_tensors="pt", truncation=True, padding=True, max_length=256).to(device)
+        with torch.no_grad():
+            out = reward_model(**inp)
+            base = float(out.logits.squeeze().item())
+        shaped = apply_reward_shaping(base, response, shaping_cfg)
+        return shaped
+
+    # Training loop (mini)
+    for epoch in range(ppo_epochs):
+        logger.info("PPO epoch %d/%d", epoch+1, ppo_epochs)
+        for i in range(0, len(prompts), batch_size):
+            batch_prompts = prompts[i:i+batch_size]
+            # generate responses
+            gen_texts = []
+            for p in batch_prompts:
+                inputs = tok_policy(p, return_tensors="pt").to(device)
+                gen = policy.generate(**inputs, max_new_tokens=shaping_cfg.get("max_new_tokens", 64), do_sample=True, temperature=shaping_cfg.get("temperature", 1.0))
+                text = tok_policy.decode(gen[0], skip_special_tokens=True)
+                gen_texts.append(text)
+            # score each
+            rewards = [score_with_reward(q, r) for q, r in zip(batch_prompts, gen_texts)]
+            # log rollouts
+            for q, r, rew in zip(batch_prompts, gen_texts, rewards):
+                rec = {"prompt": q, "response": r, "reward": rew, "epoch": epoch}
+                with open(rollouts_file, "a", encoding="utf-8") as f:
+                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")
+                # if shaped reward below threshold or moderation flagged, enqueue to HIL
+                if rew < shaping_cfg.get("flag_threshold", -1.0):
+                    enqueue_hil(q, r, reason="low_shaped_reward", metadata={"reward": rew})
+            # perform PPO step
+            try:
+                trainer.step(batch_prompts, gen_texts, rewards=rewards)
+            except TypeError:
+                # try alternative signature
+                try:
+                    trainer.step(prompts=batch_prompts, responses=gen_texts, rewards=rewards)
+                except Exception as e:
+                    logger.warning("trainer.step failed: %s", e)
+    # save policy
+    saved = out_path / "policy_post_ppo"
+    trainer.model.save_pretrained(str(saved))
+    logger.info("Saved PPO-updated policy to %s", saved)
+    return {"policy_saved": str(saved), "rollouts": str(rollouts_file)}
+
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--prompts", default="data/ppo_prompts.txt")
+    parser.add_argument("--policy", default="distilgpt2")
+    parser.add_argument("--reward_model", default="outputs/reward")
+    parser.add_argument("--out_dir", default="outputs/ppo_shaped")
+    parser.add_argument("--ppo_epochs", type=int, default=1)
+    parser.add_argument("--batch_size", type=int, default=1)
+    parser.add_argument("--shaping_cfg", default="configs/reward_shaping.yaml")
+    args = parser.parse_args()
+
+    # load prompts
+    prompts = load_prompts(args.prompts)
+    # load shaping cfg
+    import yaml
+    shaping = {}
+    if Path(args.shaping_cfg).exists():
+        shaping = yaml.safe_load(Path(args.shaping_cfg).read_text())
+    res = ppo_train(prompts, args.policy, args.reward_model, args.out_dir, shaping, ppo_epochs=args.ppo_epochs, batch_size=args.batch_size)
+    print("PPO done. Results:", res)
+
--- /dev/null
+++ b/tools/qa_dashboard_stub.py
@@ -0,0 +1,126 @@
+#!/usr/bin/env python3
+"""
+tools/qa_dashboard_stub.py
+
+Small CLI to generate QA summaries for safety/QA leads:
+ - annotator metrics
+ - gold-test pass rates
+ - HIL queue size
+ - summary of recent rollouts (from PPO shaped rollouts file)
+
+This is a stub for a dashboard exporter. Integrate with real dashboard/DB for production.
+"""
+from __future__ import annotations
+import argparse
+import csv
+import json
+from pathlib import Path
+from typing import Dict
+
+from openquill.safety.hil_queue import list_pending
+
+
+def load_annotations(path: str):
+    rows = []
+    with open(path, "r", encoding="utf-8") as f:
+        reader = csv.DictReader(f)
+        for r in reader:
+            rows.append(r)
+    return rows
+
+
+def compute_gold_stats(annotations: list, gold_prompts: set):
+    total = 0
+    pass_count = 0
+    for r in annotations:
+        p = r.get("prompt","")
+        if p in gold_prompts:
+            total += 1
+            if r.get("notes","").lower().find("gold:pass") >= 0:
+                pass_count += 1
+    return {"gold_total": total, "gold_pass": pass_count, "gold_pass_rate": (pass_count/total if total else None)}
+
+
+def recent_rollouts(rollouts_file: str, limit: int = 50):
+    p = Path(rollouts_file)
+    if not p.exists():
+        return []
+    out = []
+    with p.open("r", encoding="utf-8") as f:
+        for i, ln in enumerate(f):
+            if i >= limit:
+                break
+            try:
+                out.append(json.loads(ln))
+            except Exception:
+                continue
+    return out
+
+
+def main():
+    p = argparse.ArgumentParser()
+    p.add_argument("--annotations", default="annotations/annotations.csv")
+    p.add_argument("--gold", default="annotations/gold_tests.json")
+    p.add_argument("--rollouts", default="outputs/ppo_shaped/rollouts.jsonl")
+    p.add_argument("--out", default="qa_summary.json")
+    args = p.parse_args()
+
+    annotations = load_annotations(args.annotations) if Path(args.annotations).exists() else []
+    gold = set()
+    if Path(args.gold).exists():
+        try:
+            g = json.loads(Path(args.gold).read_text())
+            gold = {item["prompt"] for item in g}
+        except Exception:
+            gold = set()
+
+    gold_stats = compute_gold_stats(annotations, gold)
+    hil_pending = list_pending()
+    rollouts = recent_rollouts(args.rollouts, limit=50)
+
+    summary = {
+        "annotator_count": len(set([a.get("annotator","") for a in annotations])),
+        "annotations_total": len(annotations),
+        "gold_stats": gold_stats,
+        "hil_pending_count": len(hil_pending),
+        "recent_rollouts_count": len(rollouts),
+        "sample_rollouts": rollouts[:5],
+    }
+    Path(args.out).write_text(json.dumps(summary, indent=2), encoding="utf-8")
+    print("Wrote QA summary to", args.out)
+
+if __name__ == "__main__":
+    main()
+
--- /dev/null
+++ b/configs/reward_shaping.yaml
@@ -0,0 +1,42 @@
+# configs/reward_shaping.yaml
+# Conservative reward shaping configuration used by ppo_shaped.py
+
+toxicity_penalty: -6.0      # penalty for moderation-flagged responses
+length_penalty: -1.0        # penalty when response tokens exceed max_len
+max_len: 512                # preferred maximum tokens in response
+flag_threshold: -1.5        # shaped reward below which rollout is sent to HIL
+max_new_tokens: 128         # generation length during PPO rollouts
+temperature: 1.0            # sampling temperature
+repetition_penalty: -0.5    # optional repetition penalty (not implemented in baseline)
+
+# You should tune these conservatively. Strong negative penalties cause the policy to prefer safe/short responses.
--- a/.gitignore
+++ b/.gitignore
@@ -1,3 +1,6 @@
+# ignore artifacts
+outputs/
+models/
+
--- /dev/null
+++ b/scripts/promote_policy_gate.sh
@@ -0,0 +1,153 @@
+#!/usr/bin/env bash
+set -euo pipefail
+#
+# scripts/promote_policy_gate.sh
+#
+# Gate operator to promote a PPO-updated policy into staging/production.
+# Checks:
+#  - annotation gold pass rate >= threshold
+#  - reward holdout validation AUC >= threshold
+#  - HIL queue below threshold
+#  - manual human approval (interactive or CI token)
+#
+# Usage:
+#   ./scripts/promote_policy_gate.sh --annotations annotations/annotations.csv --gold annotations/gold_tests.json --reward_report outputs/reward_holdout/reward_holdout_report.json --hil_threshold 10 --auc 0.6
+
+ROOT_DIR=$(cd "$(dirname "$0")/../" && pwd)
+
+ANNOTATIONS=${ANNOTATIONS:-"annotations/annotations.csv"}
+GOLD=${GOLD:-"annotations/gold_tests.json"}
+REWARD_REPORT=${REWARD_REPORT:-"outputs/reward_holdout/reward_holdout_report.json"}
+HIL_THRESHOLD=${HIL_THRESHOLD:-10}
+AUC_THRESHOLD=${AUC_THRESHOLD:-0.60}
+GOLD_PASS_THRESHOLD=${GOLD_PASS_THRESHOLD:-0.90}
+
+function usage() {
+  echo "Usage: $0 --annotations <csv> --gold <gold.json> --reward_report <json> [--hil_threshold N] [--auc 0.6] [--gold_pass 0.9]"
+  exit 1
+}
+
+while [[ $# -gt 0 ]]; do
+  case "$1" in
+    --annotations) ANNOTATIONS="$2"; shift 2;;
+    --gold) GOLD="$2"; shift 2;;
+    --reward_report) REWARD_REPORT="$2"; shift 2;;
+    --hil_threshold) HIL_THRESHOLD="$2"; shift 2;;
+    --auc) AUC_THRESHOLD="$2"; shift 2;;
+    --gold_pass) GOLD_PASS_THRESHOLD="$2"; shift 2;;
+    *) usage;;
+  esac
+done
+
+if [ ! -f "$ANNOTATIONS" ]; then
+  echo "Annotations CSV not found: $ANNOTATIONS"; exit 2
+fi
+if [ ! -f "$GOLD" ]; then
+  echo "Gold file not found: $GOLD"; exit 2
+fi
+if [ ! -f "$REWARD_REPORT" ]; then
+  echo "Reward report not found: $REWARD_REPORT"; exit 2
+fi
+
+echo "1) Compute gold pass rate"
+GOLD_PASS=$(python - <<PY
+import json, csv, sys
+gold = {item['prompt']: item['preferred'] for item in json.load(open("$GOLD"))}
+total=0; passed=0
+with open("$ANNOTATIONS") as f:
+    r=csv.DictReader(f)
+    for row in r:
+        p=row.get('prompt','')
+        if p in gold:
+            total+=1
+            pref = row.get('preferred','').upper()
+            if pref == gold[p].upper():
+                passed+=1
+print((passed/total) if total else 0.0)
+PY
+)
+echo "Gold pass rate: $GOLD_PASS (threshold: $GOLD_PASS_THRESHOLD)"
+awk -v gp="$GOLD_PASS" -v th="$GOLD_PASS_THRESHOLD" 'BEGIN{ if (gp+0< th+0) exit 2 }'
+
+echo "2) Check reward holdout AUC"
+REWARD_AUC=$(python - <<PY
+import json
+rep=json.load(open("$REWARD_REPORT"))
+auc = rep.get('test',{}).get('roc_auc') or rep.get('val',{}).get('roc_auc') or 0.0
+print(auc)
+PY
+)
+echo "Reward AUC: $REWARD_AUC (threshold: $AUC_THRESHOLD)"
+awk -v ra="$REWARD_AUC" -v th="$AUC_THRESHOLD" 'BEGIN{ if (ra+0 < th+0) exit 3 }'
+
+echo "3) HIL queue size"
+HIL_COUNT=$(python - <<PY
+from openquill.safety.hil_queue import list_pending
+print(len(list_pending()))
+PY
+)
+echo "HIL queue size: $HIL_COUNT (threshold: $HIL_THRESHOLD)"
+awk -v hc="$HIL_COUNT" -v th="$HIL_THRESHOLD" 'BEGIN{ if (hc+0 > th+0) exit 4 }'
+
+echo "4) Manual human approval"
+read -p "Type YES to approve promotion to staging (or anything else to abort): " APPROVE
+if [ "$APPROVE" != "YES" ]; then
+  echo "Promotion aborted by operator."
+  exit 5
+fi
+
+echo "Promotion gate passed. Proceed to staging deployment (operator action)."
+exit 0
+
--- /dev/null
+++ b/docs/rhf_operational_playbook.md
@@ -0,0 +1,216 @@
+# RLHF Operational Playbook
+
+Purpose
+- This document describes day-to-day and release-time operational procedures to run RLHF safely at scale using OpenQuill.
+
+Roles
+- Annotation manager: runs annotation service, onboards annotators, monitors gold tests and quality.
+- Safety lead: reviews HIL items, adjudicates rollouts, runs red-team.
+- ML engineer: trains reward models, runs PPO pilots, prepares artifacts for release.
+- Ops: provisions infra (GPU nodes, PVCs), deploys firecracker controller, configures monitoring and alerts.
+- Legal: reviews dataset & model licenses and signs off on releases.
+
+Key components
+- Annotation service (FastAPI) + annotator_onboard / annotation_admin utilities
+- Gold tests and daily QC (tools/annotation_qc.py, tools/annotation_admin.py)
+- Reward model holdout validation (openquill/safety/reward_holdout.py)
+- PPO with reward shaping + HIL enqueue (openquill/training/ppo_shaped.py)
+- HIL queue and human review process (openquill/safety/hil_queue.py)
+- Promotion gate script that blocks policy promotion until checks pass (scripts/promote_policy_gate.sh)
+
+Operational workflow (normal day)
+1) Monitor dashboard and alerts:
+   - HIL queue size, safety flags, annotation throughput.
+   - Investigate any high-safety-flag alerts (Prometheus rules provided).
+
+2) Annotation ops:
+   - Onboard annotators using tools/annotator_onboard.py or tools/annotation_admin.py.
+   - Ensure annotators complete gold tests; monitor gold pass rates.
+   - Remove low-quality annotators and mark low-quality annotations via tools/annotation_admin.py.
+
+3) Reward model maintenance:
+   - Periodically re-train reward models on latest audited pairs.
+   - Use reward_holdout.py to evaluate holdout performance and keep track of AUC/accuracy.
+
+4) PPO pilots:
+   - Run PPO pilots on small policies (distilgpt2 or small variants) using ppo_shaped.py with conservative shaping config.
+   - Log rollouts and enqueue any low-shaped-reward or flagged responses to HIL for review.
+   - ML engineer annotates rollouts and improves reward shaping as necessary.
+
+Release workflow (per release candidate)
+1) Produce merged SFT candidate and upload to guarded storage.
+2) Collect a sufficient RLHF annotation dataset (>=200-1000 pairs depending on scope).
+3) Run tools/annotation_qc.py and tools/annotation_admin.py metrics; require gold_pass >= 0.9.
+4) Train reward model; run reward_holdout.py; require test AUC >= threshold (org-defined).
+5) Run PPO pilot with reward shaping; have safety lead review rollouts in HIL queue.
+6) Run quantize/validate pipeline and long-context benchmarks.
+7) Execute scripts/finalize_release_pipeline.py and scripts/validate_release_and_publish.sh; resolve issues.
+8) Obtain signed docs/legal_signoff.json and run scripts/publish_final_weights.py to publish to guarded storage or HF private repo.
+
+Conservative reward shaping tips
+- Penalize any moderation-flagged output heavily (toxicity_penalty).
+- Penalize long and rambling outputs (length_penalty) to reduce ungrounded content.
+- Penalize overly certain statements when retrieval confidence is low (if using retrieval signals).
+- Keep penalties relatively large (absolute) during pilot; relax iteratively with human review.
+
+HIL human review process
+- Safety lead reviews queued items daily.
+- Classify items as: safe, unsafe, needs_rewrite, needs_policy_update.
+- Maintain audit logs and remediation tickets for unsafe items.
+
+Monitoring & SLOs
+- p95 latency for quantized 7B tier, safety spike alerts, HIL queue thresholds and annotator throughput.
+- On-call rotation covering safety & infra.
+
+Incident & rollback
+- If a deployed policy shows safety regressions:
+  - Immediately roll back to the previous policy.
+  - Create incident ticket, run root-cause analysis, and update reward shaping / dataset if needed.
+
+This playbook is a living document; adapt to your organizational compliance and safety requirements.
+
--- 
2.40.1
